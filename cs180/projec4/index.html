<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project 4</title>
    <link rel="stylesheet" href="../template/style.css"> <!--path to style sheet "../style.css"-->
    <link
        rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css"
        integrity="sha512-…(hash)…" crossorigin="anonymous"
        referrerpolicy="no-referrer"
    />
</head>
<body>
    <!-- top menu-->
    <header class="sticky_header">
        <div class="sticky_header_container">
            <h1><a href="../index.html">CS 180</a></h1>
            <nav>
                <ul>
                    <li> <h1><a>Project 4 &nbsp;<i class="fas fa-chevron-down"></i></a></h1>
                        <ul>
                            <li><a href="../project0/index.html">Project 0</a></li>
                            <li><a href="../project1/index.html">Project 1</a></li>
                            <li><a href="../project2/index.html">Project 2</a></li>
                            <li><a href="../project3/index.html">Project 3</a></li>
                            <li><a href="../project4/index.html">Project 4</a></li>
                        </ul>
                    </li>
                </ul>
            </nav>
        </div>
    </header>

    <!-- title -->
    <header class="autumn">
        <div class="autumn_inner">
            <h1>Neural Radiance Field!</h1>
        </div>
    </header>

    <div class="main_container">
        <!-- siderbar -->
        <div class="sidebar">
            <h2>Content</h2>
            <h3>Part 0</h3>
            <a href="#part0">Camera Calibration and 3D Scanning</a>
            <h3>Part 1</h3>
            <a href="#part1">Fit a Neural Field to a 2D Image</a>
            <h3>Part 2</h3>
            <a href="#part2">Fit a Neural Radiance Field from Multi-view Images</a>
            <h3>Part 2.6</h3>
            <a href="#part2.6">Training NeRF with My Own Data</a>
        </div>

        <main class="wrap" style="max-width: 80%;">
            <section class="card" id="part0">
                <h2>Part 0: Camera Calibration and 3D Scanning</h2>
                <hr>
                <p class="sub">
                    In this part, I have created the dataset for the Neural Radiance Field model, which should
                    contains the images taken around the target object, and the camera-to-world (c2w) matrix for
                    the camera pose of each images.
                </p>
                <p class="sub">
                    To estimate the camera pose, I need first to calibrate the camera, which means to calculate 
                    the intrinsics (K) and disortion coefficients of my camera. This is implemented by using the 
                    same camera and same camera parameters to take pictures of the calibration tags (ArUco), and 
                    use the Aruco Detector from OpenCV to detect the corner positions of the ArUco tags in taken 
                    pictures. Together with the corresponding object 3D positions, use <code>cv2.calibrateCamera()</code>
                    to calculate the camera intrinsics and distortion coefficients.
                </p>
                <p class="sub">
                    Next, for each dataset image, with the camera intrinsics, distortion coefficients calculated, 
                    and the corresponding point positions in the images and in real world, use <code>cv2.solvePnP()</code>
                    to calculate the <code>rvec</code> (3x1 rotation vector 'r') and <code>tvec</code> (3x1 translation vector 't')
                    of the image. Convert the rotation vector to 3x3 with <code>R = cv2.Rodrigues(rvec)</code>. 
                    The 4x4 world-to-camera (w2c) matrix is [[R, t], [0, 1]], and the 4x4 camera-to-world (c2w) 
                    is the inverse of w2c. <br><br>
                </p>
                <div class="matrix-wrapper">
                    <p>c2w&nbsp;=&nbsp;</p>
                    <table class="matrix">
                        <tr><td>R<sup>T</sup></td><td>-R<sup>T</sup>t</td></tr>
                        <tr><td>0</td><td>1</td></tr>
                    </table>
                </div>
                <p class="sub">
                    With the images and corresponding c2ws, I can use <i>viser</i> to visualize the camera pose frustum in 3D:
                </p>
                <div class="grid">
                    <div class="col-1"></div>
                    <div class="col-5">
                        <figure>
                            <img src="./out/p0_viser1.png" alt="pose visualization 1" data-lightbox>
                            </figcaption>
                        </figure>
                    </div>
                    <div class="col-5">
                        <figure>
                            <img src="./out/p0_viser2.png" alt="pose visualization 2" data-lightbox>
                            </figcaption>
                        </figure>
                    </div>
                    <div class="col-1"></div>
                </div>
                <p class="sub">
                    It can be observed, the images are located as a half dome, the cameras are facing to the center,
                    where the target object should be located at.
                </p>
                <p class="sub">
                    In the end, I randomly assign the (image, c2w) pairs to the training set and valid set, with a ratio
                    of 9:1. I also saved the focal length in my dataset, where I assume <code>f<sub>x</sub> = f<sub>y</sub></code>.
                    Therefore, <code>focal = (K[0,0] + K[1,1]) / 2</code>
                </p>
            </section>

            <section class="card" id="part1">
                <h2>Part 1: Fit a Neural Field to a 2D Image</h2>
                <hr>
                <p class="sub">
                    Before model the NeRF in 3D, I first model a Neural Field in 2D. This model will take in the pixel positions
                    in an image to predict the color on that pixel. My MLP model architecture is following the image below. I first
                    use positional encoding to extend input 2-dimensional xy position to 2*(L+1) dimensions of frequency, then send 
                    encoded input to three linear layers with a channel size (default to ch=256), add a Relu layer after each linear layer.
                    Finally, I covert the data from 256 dimensions back to 3 dimensions (rgb), followed with a sigmoid function to 
                    constraint the results in [0, 1] range. I use the squared error loss (MSE) as loss function, and Adam as optimizer,
                    with learning rate lr=1e-2.
                </p>
                <figure>
                    <img src="./media/p1_mlp_img.jpg" alt="NeF MLP layers" data-lightbox>
                </figure>
                <hr>
                <h3 style="margin-top: 20px; margin-bottom: 20px;">
                    Fox (ch=256, L=10, lr=0.01)
                </h3>
                <figure style="display: block; margin-left: auto; margin-right: auto; width: 50%;">
                    <img src="./media/fox.jpg" alt="Fox original" data-lightbox>
                    <figcaption>
                        <strong>Orignal Image</strong><br>
                        <br>
                    </figcaption>
                </figure>
                <figure>
                    <img src="./out/p1_fox_training_progression.jpg" alt="Training progress" data-lightbox>
                    </figcaption>
                </figure>
                <figure>
                    <img src="./out/p1_fox_training_curve.jpg" alt="Training curve" data-lightbox>
                    </figcaption>
                </figure>
                <hr>
                <h3 style="margin-top: 20px; margin-bottom: 20px;">
                    Berkeley (ch=256, L=10, lr=0.01)
                </h3>
                <figure style="display: block; margin-left: auto; margin-right: auto; width: 50%;">
                    <img src="./media/berkeley.jpg" alt="Fox original" data-lightbox>
                    <figcaption>
                        <strong>Orignal Image</strong><br>
                        <br>
                    </figcaption>
                </figure>
                <figure>
                    <img src="./out/p1_berkeley_training_progression.jpg" alt="Training progress" data-lightbox>
                    </figcaption>
                </figure>
                <figure>
                    <img src="./out/p1_berkeley_training_curve.jpg" alt="Training curve" data-lightbox>
                    </figcaption>
                </figure>
                <hr>
                <p class="sub">
                    To learn the affect of changing channel size and L value, I also run the fox image, with model 
                    (ch=256, L=2), (ch=64, L=2) and (ch=64, L=10). From the result, I found that a higher channel size
                    make the image more detailed in small features like the hairs, a higher L-value makes the image
                    transfer smoother on the edges and reseves the shape more estimate.
                </p>
                <figure style="display: block; margin-left: auto; margin-right: auto; width: 80%;">
                    <img src="./out/p0_fox_2x2_grid.jpg" alt="2x2 grid" data-lightbox>
                </figure>
            </section>

            <section class="card" id="part2">
                <h2>Part 2: Fit a Neural Radiance Field from Multi-view Images</h2>
            </section>
        </main>

    </div>




</body>












<!--Lightbox-->
<div class="lightbox" id="lightbox" aria-hidden="true" role="dialog">
    <img alt="" id="lb-img" style="display:none">
    <video id="lb-video" style="display:none" controls></video>
</div>

<!--Lightbox script (reused)-->
<script>
    const lb = document.getElementById('lightbox');
    const lbImg = document.getElementById('lb-img');
    const lbVideo = document.getElementById('lb-video');

    function openLightbox(el) {
        const isVideo = el.tagName.toLowerCase() === 'video' || (el.src && el.src.endsWith('.mp4'));
        lb.classList.add('open');

        if (isVideo) {
        lbVideo.src = el.src;
        lbVideo.style.display = 'block';
        lbImg.style.display = 'none';
        } else {
        lbImg.src = el.src;
        lbImg.alt = el.alt || '';
        lbImg.style.display = 'block';
        lbVideo.style.display = 'none';
        }
        lb.setAttribute('aria-hidden', 'false');
    }

    function closeLightbox() {
        lb.classList.remove('open');
        lbImg.src = '';
        lbVideo.pause();
        lbVideo.removeAttribute('src');
        lb.setAttribute('aria-hidden', 'true');
    }

    document.addEventListener('click', e => {
        const t = e.target;
        if (t.matches('[data-lightbox]')) {
        openLightbox(t);
        } else if (t.id === 'lightbox') {
        closeLightbox();
        }
    });

    document.addEventListener('keydown', e => {
        if (e.key === 'Escape' && lb.classList.contains('open')) {
        closeLightbox();
        }
    });
</script>